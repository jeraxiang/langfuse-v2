{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from typing import List, Dict\n",
    "from langfuse import Langfuse\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "\n",
    "from dateutil import parser\n",
    "import pytz\n",
    "from src.blob_storage import BlobStorageHandler\n",
    "import tempfile\n",
    "import glob\n",
    "import logging\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Clear existing handlers to avoid duplicate logs\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()\n",
    "\n",
    "# Add a handler to display log in notebook directly\n",
    "handler = logging.StreamHandler()\n",
    "handler.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(handler)\n",
    "\n",
    "# helper function\n",
    "def pydantic_list_to_dataframe(pydantic_list):\n",
    "    \"\"\"\n",
    "    Convert a list of pydantic objects to a pandas dataframe.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for item in pydantic_list:\n",
    "        data.append(item.dict())\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def convert_to_utc(date_str: str, date_type: str, local_tz=dt.timezone(dt.timedelta(hours=9))):\n",
    "    local_date = dt.datetime.strptime(date_str, \"%Y-%m-%d\")  # Convert string to datetime\n",
    "    local_date = dt.datetime(local_date.year, local_date.month, local_date.day, \n",
    "                                0, 0, 0, 0, tzinfo=local_tz)  # Set time to 00:00:00\n",
    "    if date_type == 'to':\n",
    "        local_date = local_date - dt.timedelta(microseconds=1) # End of yesterday\n",
    "    return local_date.astimezone(dt.timezone.utc)  # Convert to UTC\n",
    "\n",
    "def get_timestamps(from_timestamp=None, to_timestamp=None, local_tz=dt.timezone(dt.timedelta(hours=9))):\n",
    "\n",
    "    if not from_timestamp:\n",
    "        from_timestamp = dt.datetime.strftime(dt.datetime.now() - dt.timedelta(days=1), \"%Y-%m-%d\")\n",
    "    from_timestamp = convert_to_utc(from_timestamp, date_type='from', local_tz=local_tz)\n",
    "\n",
    "    if not to_timestamp:\n",
    "        to_timestamp = dt.datetime.strftime(dt.datetime.now(), \"%Y-%m-%d\")\n",
    "    else:\n",
    "        to_timestamp = dt.datetime.strftime(dt.datetime.strptime(to_timestamp, \"%Y-%m-%d\") + dt.timedelta(days=1), \"%Y-%m-%d\")\n",
    "    to_timestamp = convert_to_utc(to_timestamp, date_type='to', local_tz=local_tz)\n",
    "\n",
    "    return from_timestamp, to_timestamp\n",
    "\n",
    "def expend_metadata(df, metadata_keys: List[str] | None = None):\n",
    "        # Select specific keys to extract\n",
    "        # metadata_keys = ['channel', 'user_email', 'department_name', 'azure_user_id', 'question_uuid', 'costs']\n",
    "\n",
    "        if metadata_keys is None or metadata_keys == []:\n",
    "            # Convert None/NaN to an empty dictionary before expanding\n",
    "            df_expanded = df['metadata'].apply(lambda x: pd.Series(x) if isinstance(x, dict) else pd.Series(dtype='object'))\n",
    "\n",
    "            # Concatenate original DataFrame (excluding 'metadata') with expanded columns\n",
    "            df = pd.concat([df, df_expanded], axis=1)\n",
    "        else:\n",
    "            # Create new columns with selected keys\n",
    "            # df['metadata'].values[0]\n",
    "            for key in metadata_keys:\n",
    "                # if key not in \n",
    "                df[key] = df['metadata'].apply(lambda x: x.get(key, None) if isinstance(x, dict) else None)  # Check if x is a dict\n",
    "\n",
    "        # Drop the original 'metadata' column if not needed\n",
    "        df = df.drop(columns=['metadata'])\n",
    "        return df\n",
    "\n",
    "def convert_to_jst(utc_dt):\n",
    "    # Original datetime in UTC\n",
    "    # utc_time = dt.datetime.fromisoformat('2025-03-15 15:00:00+00:00')\n",
    "\n",
    "    # Convert to JST (UTC+9)\n",
    "    jst_time = utc_dt.astimezone(dt.timezone(dt.timedelta(hours=9)))\n",
    "\n",
    "    return jst_time\n",
    "\n",
    "def datetime_to_iso8601(org_dt: dt.datetime) -> str:\n",
    "    iso_str = org_dt.replace(microsecond=0).isoformat().replace(\"+00:00\", \"Z\")\n",
    "    return iso_str\n",
    "\n",
    "\n",
    "\n",
    "class LangfuseAPI(object):\n",
    "    def __init__(self, public_key: str, secret_key: str, host: str):\n",
    "        self.host = host\n",
    "        self.public_key = public_key\n",
    "        self.secret_key = secret_key\n",
    "        self.langfuse = Langfuse(public_key = public_key, secret_key = secret_key, host = host)\n",
    "\n",
    "    def fetch_traces(\n",
    "        self, \n",
    "        limit: int = 100, \n",
    "        page: int = 1, \n",
    "        user_id: str | None = None, \n",
    "        from_timestamp: str | None = None, \n",
    "        to_timestamp: str | None =None, \n",
    "        max_traces: int | None = None,\n",
    "        metadata_keys: List[str] | None = None  # default keys to extract\n",
    "    ):\n",
    "\n",
    "        from_timestamp, to_timestamp = get_timestamps(from_timestamp, to_timestamp)\n",
    "        logger.info(f\"Collect data from JST {convert_to_jst(from_timestamp)} to {convert_to_jst(to_timestamp)}\")\n",
    "        # logger.debug(f\"Collect data from uTC {from_timestamp} to {to_timestamp}\")\n",
    "        all_traces = []\n",
    "        while True:\n",
    "            # logger.debug(f\"Page#{page}\")\n",
    "            traces = self.langfuse.fetch_traces(limit=limit, page=page, user_id=user_id, from_timestamp=from_timestamp, to_timestamp=to_timestamp)\n",
    "            all_traces.extend(traces.data)\n",
    "            \n",
    "            if len(traces.data) == 0:\n",
    "                logger.info(\"No more traces found.\")\n",
    "                break\n",
    "\n",
    "            if page % 10 == 0:\n",
    "                logger.info(f\"Page#{page}, {traces.data[0].timestamp} ~ {traces.data[-1].timestamp}\")\n",
    "\n",
    "            if len(traces.data) < limit:\n",
    "                logger.info(f\"All traces have been collected as the number of records is less than limit {limit} per page.\")\n",
    "                logger.info(f\"Page#{page}, {traces.data[0].timestamp} ~ {traces.data[-1].timestamp}\")\n",
    "                break\n",
    "            \n",
    "            if max_traces is not None and len(all_traces) >= max_traces:\n",
    "                logger.info(f\"The number of traces has been collected exceeds the total limit {max_traces}\")\n",
    "                break\n",
    "            \n",
    "            page += 1\n",
    "        \n",
    "        logger.info(f\"Retrieved {len(all_traces)} traces.\")\n",
    "        all_traces_df = pydantic_list_to_dataframe(all_traces)\n",
    "\n",
    "        if len(all_traces) > 0 and 'metadata' in all_traces_df.columns:\n",
    "            all_traces_df = expend_metadata(all_traces_df, metadata_keys)\n",
    "        return all_traces_df\n",
    "    def fetch_observations(\n",
    "        self, \n",
    "        limit: int = 100, \n",
    "        page: int = 1, \n",
    "        user_id: str | None = None, \n",
    "        from_timestamp: str | None = None, \n",
    "        to_timestamp: str | None =None, \n",
    "        max_traces: int | None = None,\n",
    "        metadata_keys: List[str] | None = None  # default keys to extract\n",
    "    ):\n",
    "\n",
    "        from_timestamp, to_timestamp = get_timestamps(from_timestamp, to_timestamp)\n",
    "        logger.info(f\"Collect data from JST {convert_to_jst(from_timestamp)} to {convert_to_jst(to_timestamp)}\")\n",
    "        # logger.debug(f\"Collect data from uTC {from_timestamp} to {to_timestamp}\")\n",
    "        all_data = []\n",
    "        while True:\n",
    "            # logger.debug(f\"Page#{page}\")\n",
    "            obs = self.langfuse.fetch_observations(limit=limit, page=page, user_id=user_id, from_start_time=from_timestamp, to_start_time=to_timestamp)\n",
    "            all_data.extend(obs.data)\n",
    "            #print(obs.data)\n",
    "\n",
    "            if len(obs.data) == 0:\n",
    "                logger.info(\"No more data found.\")\n",
    "                break\n",
    "            \n",
    "            if page % 10 == 0:\n",
    "                logger.info(f\"Page#{page}, {obs.data[0].start_time} ~ {obs.data[-1].start_time}\")\n",
    "\n",
    "            if len(obs.data) < limit:\n",
    "                logger.info(f\"All data have been collected as the number of records is less than limit {limit} per page.\")\n",
    "                logger.info(f\"Page#{page}, {obs.data[0].start_time} ~ {obs.data[-1].start_time}\")\n",
    "                break\n",
    "            \n",
    "            if max_traces is not None and len(all_data) >= max_traces:\n",
    "                logger.info(f\"The number of data has been collected exceeds the total limit {max_traces}\")\n",
    "                break\n",
    "            \n",
    "            page += 1\n",
    "        \n",
    "        logger.info(f\"Retrieved {len(all_data)} data.\")\n",
    "        all_df = pydantic_list_to_dataframe(all_data)\n",
    "\n",
    "        if len(all_data) > 0 and 'metadata' in all_df.columns:\n",
    "            all_df = expend_metadata(all_df, metadata_keys)\n",
    "        return all_df\n",
    "    def fetch_scores(\n",
    "        self, \n",
    "        limit: int = 100, \n",
    "        page: int = 1, \n",
    "        user_id: str | None = None, \n",
    "        from_timestamp: str | None = None, \n",
    "        to_timestamp: str | None =None, \n",
    "        max_traces: int | None = None,\n",
    "    ):\n",
    "        from_timestamp, to_timestamp = get_timestamps(from_timestamp, to_timestamp)\n",
    "        logger.info(f\"Collect data from JST {convert_to_jst(from_timestamp)} to {convert_to_jst(to_timestamp)}\")\n",
    "        # logger.debug(f\"Collect data from uTC {from_timestamp} to {to_timestamp}\")\n",
    "        all_scores = []\n",
    "        while True:\n",
    "            logger.info(f\"Page#{page}\")\n",
    "            \n",
    "            # Define parameters\n",
    "            params = {\n",
    "                \"page\": page,\n",
    "                \"limit\": limit,\n",
    "                \"userId\": user_id,\n",
    "                \"fromTimestamp\": datetime_to_iso8601(from_timestamp),  # ISO 8601 format\n",
    "                \"toTimestamp\": datetime_to_iso8601(to_timestamp)\n",
    "            }\n",
    "\n",
    "            # Example: Fetch scores\n",
    "            response = requests.get(\n",
    "                f\"{self.host}/api/public/scores\",\n",
    "                auth=HTTPBasicAuth(self.public_key, self.secret_key),\n",
    "                params=params\n",
    "            )\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                data = response.json().get(\"data\", [])\n",
    "                all_scores.extend(data)\n",
    "                \n",
    "                if len(data) < limit:\n",
    "                    logger.info(\"All data have been collected as the number of records is less than limit per page.\")\n",
    "                    break\n",
    "                \n",
    "                if max_traces is not None and len(all_scores) >= max_traces:\n",
    "                    logger.info(\"The number of data has been collected exceeds the total limit\")\n",
    "                    break\n",
    "                \n",
    "            else:\n",
    "                logger.error(f\"Error fetching scores: {response.status_code} - {response.text}\")\n",
    "                break\n",
    "            page += 1\n",
    "        score_df = pd.DataFrame(all_scores)\n",
    "        return score_df\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "function_names_mapping = {\n",
    "    \"meeting_minutes\": [\"minutes\"],\n",
    "    \"jera_knowledge\": [\"jera_knowledge\"],\n",
    "    \"file_difference\": [\"file_diff\"],\n",
    "    \"presentation_generator\": [\"ppt_generator\"],\n",
    "    \"summarize\": [\"summarize\"],\n",
    "    \"chat_with_image\": [\"image_to_text\"],\n",
    "    \"docx_translator\": [\"docx_file_translator\"],\n",
    "    \"pptx_translator\": [\"pptx_file_translator\"],\n",
    "    \"pdf_translator\": [\"pdf_file_translator\"],\n",
    "    \"txt_translator\": [\"txt_file_translator\"],\n",
    "    \"chat_with_file\": [\"rag\"],\n",
    "    \"speech_generator\": [\"speech_draft_generator\"],\n",
    "    \"meeting_setup\": [\"meeting_setup\"],\n",
    "    \"text_translator\": [\"text_translator\", \"transText_extractor\"],\n",
    "    \"simple_web_search\":[\"simple_web_searcher\"],\n",
    "    \"research_report\":[\"research_report\"],\n",
    "    \"outlook_redirect\": [\"outlook_redirect\"],\n",
    "    \"outlook_triggered\": [\"outlook_adaptive_card_triggered\"],\n",
    "}\n",
    "\n",
    "\n",
    "def upload_to_blob(project:str, container_name, fpath: str, overwrite: bool = False):\n",
    "    conn_str = os.environ[f\"{project}_CONNECTION_STRING_PROD\"]\n",
    "    bsh = BlobStorageHandler(conn_str=conn_str, container_name=container_name)\n",
    "\n",
    "    file_name = os.path.basename(fpath)\n",
    "    to_file_path = os.path.join(file_name)\n",
    "\n",
    "    if bsh.blob_exists(to_file_path) and not overwrite:\n",
    "        print(f\"`{to_file_path}` already exists in {container_name}.\")\n",
    "    else:\n",
    "        bsh.upload_file(fpath, to_file_path, overwrite)\n",
    "        print(f\"`{to_file_path}` uploaded to blob {container_name}.\")\n",
    "\n",
    "def convert_to_local(utc_time: str, formatter: str = \"%Y-%m-%d %H:%M:%S\"):\n",
    "    utc_time = dt.datetime.strptime(utc_time[:19], \"%Y-%m-%dT%H:%M:%S\")\n",
    "    # Set the timezone to UTC for the datetime object\n",
    "    utc_time = utc_time.replace(tzinfo=pytz.UTC)\n",
    "    # Convert to Japan Standard Time (JST)\n",
    "    jst_time = utc_time.astimezone(pytz.timezone(\"Asia/Tokyo\"))\n",
    "    jst_time = dt.datetime.strftime(jst_time, formatter)\n",
    "\n",
    "    return jst_time\n",
    "\n",
    "def format_datetime(val):\n",
    "    try:\n",
    "        dt = parser.isoparse(val)\n",
    "        return dt.strftime('%Y-%m-%dT%H:%M:%S.%f')[:-3] + 'Z'\n",
    "    except Exception as e:\n",
    "        return val  # or raise, log, or return None if desired\n",
    "\n",
    "def add_new_name_by_quid(qid_to_new_names: dict, qid: str, new_name: str):\n",
    "    if qid not in qid_to_new_names:\n",
    "        qid_to_new_names[qid] = new_name\n",
    "    else:\n",
    "        logger.info(f\"Question ID: {qid} already exists, the new function name is {qid_to_new_names[qid]}\")\n",
    "    return qid_to_new_names\n",
    "\n",
    "def rename_funtion_name(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    qid_to_new_fname = {}\n",
    "    qid_col_name = \"question_uuid\"\n",
    "    qid_to_org_fnames = dict(df[[\"name\", qid_col_name]].groupby(qid_col_name)['name'].apply(lambda x: sorted(set(x))).reset_index().values)\n",
    "    qid_to_traceid =  dict(df[[\"id\", qid_col_name]].groupby([qid_col_name])['id'].apply(list).reset_index().values)\n",
    "\n",
    "    for qid, org_fn in qid_to_org_fnames.items():\n",
    "        channel = org_fn[0].split(\"_\")[0]\n",
    "        org_f_names = [\"_\".join(i.split(\"_\")[1:]) for i in org_fn]\n",
    "        \n",
    "        # function names\n",
    "        find_agent = False\n",
    "        for new_f_name, mpping_org_f_names in function_names_mapping.items():\n",
    "            if len([org_fn for org_fn in org_f_names if org_fn in mpping_org_f_names]) > 0:\n",
    "                find_agent = True\n",
    "                add_new_name_by_quid(qid_to_new_fname, qid, new_f_name)\n",
    "                break\n",
    "\n",
    "        # sob function\n",
    "        if not find_agent:\n",
    "            if org_f_names == ['language_detection']:\n",
    "                find_agent = True\n",
    "                add_new_name_by_quid(qid_to_new_fname, qid, 'other')\n",
    "            else:\n",
    "                find_agent = True\n",
    "                # if org_f_names == ['sob'] or org_f_names == ['is_outlook_meeting']:\n",
    "                add_new_name_by_quid(qid_to_new_fname, qid, 'sob')\n",
    "\n",
    "        if not find_agent:\n",
    "            logger.info(f\"channel: {channel}, Question ID: {qid}, Function Names: {org_f_names}, qid_to_traceid: {qid_to_traceid[qid]}\")\n",
    "\n",
    "    df['agent_name'] = df['question_uuid'].astype(str).map(qid_to_new_fname)\n",
    "    return df\n",
    "\n",
    "def add_whisper_cost(trace_df, obs_df):\n",
    "    trace_df = trace_df.copy()\n",
    "    obs_df = obs_df.copy()\n",
    "    trace_mm = trace_df[trace_df[\"agent_name\"] ==\"meeting_minutes\"][[\n",
    "        \"id\", \"question_uuid\", \"audio_length\", \"timestamp\"]].dropna().drop_duplicates([\"question_uuid\", \"audio_length\"])\n",
    "\n",
    "    data = []\n",
    "    for i in range(len(trace_mm)):\n",
    "        obs_id = trace_mm[\"id\"].values[i] + \"-tts\"\n",
    "        if obs_id in set(obs_df[\"id\"]):\n",
    "            print(f\"{obs_id} already exists\")\n",
    "        else:\n",
    "            tmp = {\n",
    "                \"id\": trace_mm[\"id\"].values[i] + \"-tts\",\n",
    "                \"name\": \"AgentExecutor\",\n",
    "                \"startTime\": trace_mm[\"timestamp\"].values[i],\n",
    "                \"endTime\": trace_mm[\"timestamp\"].values[i],\n",
    "                \"parentObservationId\": None,\n",
    "                \"type\": \"TTS\",\n",
    "                \"model\": \"whisper\",\n",
    "                \"completionTokens\": 0,\n",
    "                \"promptTokens\": 0,\n",
    "                \"totalTokens\": 0,\n",
    "                \"version\": None,\n",
    "                \"traceId\": trace_mm[\"id\"].values[i],\n",
    "                \"totalCost\": 0.006*trace_mm[\"audio_length\"].values[i]/(60000),\n",
    "                \"input\": None,\n",
    "                \"date\": convert_to_local(trace_mm[\"timestamp\"].values[i])[:10]\n",
    "            }\n",
    "            data.append(tmp)\n",
    "            logger.debug(tmp)\n",
    "    tmp = pd.DataFrame(data)\n",
    "    \n",
    "    obs_df = pd.concat([obs_df, tmp], ignore_index=True).sort_values(\"startTime\", ascending=False)\n",
    "    return obs_df\n",
    "\n",
    "\n",
    "def add_specific_datetime_col(df, datetime_col:str, format: str = '%Y-%m', new_col: str = 'year_month'):\n",
    "    df = df.copy()\n",
    "\n",
    "    df[datetime_col] = pd.to_datetime(df[datetime_col], format='mixed', utc=True)\n",
    "    # 3. Convert to JST (UTC+9)\n",
    "    df[datetime_col] = df[datetime_col].dt.tz_convert('Asia/Tokyo')\n",
    "\n",
    "    # 4. Create year-month column\n",
    "    df[new_col] = df[datetime_col].dt.strftime(format)\n",
    "\n",
    "    # Step 4: Convert JST timestamp to ISO format string (optional: overwrite or add new column)\n",
    "    df[datetime_col] = df[datetime_col].dt.strftime('%Y-%m-%dT%H:%M:%S.%f%z')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def load_from_blob(project:str, container_name, from_file_path: str):\n",
    "    conn_str = os.environ[f\"{project}_CONNECTION_STRING_PROD\"]\n",
    "    bsh = BlobStorageHandler(conn_str=conn_str, container_name=container_name)\n",
    "\n",
    "    if not bsh.blob_exists(from_file_path):\n",
    "        logger.info(f\"`{from_file_path}` does not exist in {container_name}.\")\n",
    "        return None\n",
    "    else:\n",
    "        # Create a temporary directory\n",
    "        with tempfile.TemporaryDirectory() as tmpdir:\n",
    "            file_name = from_file_path.split(\"/\")[-1]\n",
    "            to_file_path = os.path.join(tmpdir, file_name)\n",
    "            bsh.download_file(from_file_path, to_file_path)\n",
    "            logger.info(f\"save data to temporary place`{to_file_path}`.\")\n",
    "            df = pd.read_csv(to_file_path)\n",
    "            return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGFUSE_PUBLIC_KEY = \"pk-lf-8181225a-ebfd-43bb-9d7e-e0087597db6c\"\n",
    "LANGFUSE_SECRET_KEY = \"sk-lf-0ac2ca59-6836-45fd-8471-93d9c2b70d48\"\n",
    "LANGFUSE_HOSTNAME = \"https://langfuse.dev.jera-stg.com\"\n",
    "langfuseapi = LangfuseAPI(public_key=LANGFUSE_PUBLIC_KEY, secret_key=LANGFUSE_SECRET_KEY, host=LANGFUSE_HOSTNAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 10:36:07,601 - root - INFO - Fetch data on 2025-04-14\n",
      "2025-04-18 10:36:07,603 - root - INFO - Collect data from JST 2025-04-14 00:00:00+09:00 to 2025-04-14 23:59:59.999999+09:00\n",
      "2025-04-18 10:36:08,574 - root - INFO - Page#10, 2025-04-14 12:01:33.996000+00:00 ~ 2025-04-14 12:01:23.220000+00:00\n",
      "2025-04-18 10:36:09,332 - root - INFO - Page#20, 2025-04-14 10:27:28.706000+00:00 ~ 2025-04-14 09:56:33.709000+00:00\n",
      "2025-04-18 10:36:10,158 - root - INFO - Page#30, 2025-04-14 08:44:44.338000+00:00 ~ 2025-04-14 08:34:16.379000+00:00\n",
      "2025-04-18 10:36:11,175 - root - INFO - Page#40, 2025-04-14 07:41:47.543000+00:00 ~ 2025-04-14 07:34:56.561000+00:00\n",
      "2025-04-18 10:36:12,117 - root - INFO - Page#50, 2025-04-14 06:38:11.667000+00:00 ~ 2025-04-14 06:35:20.058000+00:00\n",
      "2025-04-18 10:36:13,158 - root - INFO - Page#60, 2025-04-14 05:52:08.667000+00:00 ~ 2025-04-14 05:51:16.415000+00:00\n",
      "2025-04-18 10:36:14,094 - root - INFO - Page#70, 2025-04-14 05:09:51.585000+00:00 ~ 2025-04-14 05:09:38.744000+00:00\n",
      "2025-04-18 10:36:15,267 - root - INFO - Page#80, 2025-04-14 04:43:45.726000+00:00 ~ 2025-04-14 04:38:10.015000+00:00\n",
      "2025-04-18 10:36:16,291 - root - INFO - Page#90, 2025-04-14 04:04:58.418000+00:00 ~ 2025-04-14 03:32:19.540000+00:00\n",
      "2025-04-18 10:36:17,406 - root - INFO - Page#100, 2025-04-14 02:34:18.312000+00:00 ~ 2025-04-14 02:34:03.369000+00:00\n",
      "2025-04-18 10:36:18,592 - root - INFO - Page#110, 2025-04-14 01:59:08.502000+00:00 ~ 2025-04-14 01:58:59.402000+00:00\n",
      "2025-04-18 10:36:19,783 - root - INFO - Page#120, 2025-04-14 01:44:46.068000+00:00 ~ 2025-04-14 01:41:31.634000+00:00\n",
      "2025-04-18 10:36:21,264 - root - INFO - Page#130, 2025-04-14 00:52:23.003000+00:00 ~ 2025-04-14 00:50:39.712000+00:00\n",
      "2025-04-18 10:36:22,733 - root - INFO - Page#140, 2025-04-14 00:13:56.113000+00:00 ~ 2025-04-14 00:07:57.597000+00:00\n",
      "2025-04-18 10:36:24,029 - root - INFO - Page#150, 2025-04-13 22:13:03.221000+00:00 ~ 2025-04-13 15:00:04.881000+00:00\n",
      "2025-04-18 10:36:24,132 - root - INFO - All data have been collected as the number of records is less than limit 100 per page.\n",
      "2025-04-18 10:36:24,134 - root - INFO - Page#151, 2025-04-13 15:00:04.881000+00:00 ~ 2025-04-13 15:00:04.020000+00:00\n",
      "2025-04-18 10:36:24,135 - root - INFO - Retrieved 15006 data.\n"
     ]
    }
   ],
   "source": [
    "limit = 100\n",
    "page = 1\n",
    "max_traces = None\n",
    "user_id = None\n",
    "metadata_keys = None\n",
    "\n",
    "dts = [\n",
    "    \"2025-04-14\"\n",
    "]\n",
    "\n",
    "for target_day in dts:\n",
    "    logger.info(f\"Fetch data on {target_day}\")\n",
    "    obs_df = langfuseapi.fetch_observations(\n",
    "        limit=limit, \n",
    "        page=page, \n",
    "        user_id=user_id, \n",
    "        from_timestamp=target_day, \n",
    "        to_timestamp=target_day,\n",
    "        metadata_keys=metadata_keys\n",
    "    )\n",
    "    obs_df = obs_df[[\"id\",\"name\",\"startTime\",\"endTime\",\"parentObservationId\",\"type\",\"model\",\"completionTokens\",\"promptTokens\",\"version\",\"traceId\",\"input\",'calculatedInputCost', 'calculatedOutputCost', 'calculatedTotalCost']]\n",
    "    obs_df = obs_df[obs_df[\"calculatedTotalCost\"] > 0.0]\n",
    "    obs_df.to_csv(f\"data/yui/obs/{target_day}.csv\", index=False, quoting=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 10:35:30,851 - root - INFO - Fetch data on 2025-04-14\n",
      "2025-04-18 10:35:30,853 - root - INFO - Collect data from JST 2025-04-14 00:00:00+09:00 to 2025-04-14 23:59:59.999999+09:00\n",
      "2025-04-18 10:35:30,855 - root - INFO - Page#1\n",
      "2025-04-18 10:35:31,032 - root - INFO - All data have been collected as the number of records is less than limit per page.\n"
     ]
    }
   ],
   "source": [
    "limit = 100\n",
    "page = 1\n",
    "max_traces = None\n",
    "user_id = None\n",
    "\n",
    "dts = [\n",
    "    \"2025-04-14\"\n",
    "]\n",
    "\n",
    "for target_day in dts:\n",
    "    logger.info(f\"Fetch data on {target_day}\")\n",
    "    score_df = langfuseapi.fetch_scores(\n",
    "        limit=limit, \n",
    "        page=page, \n",
    "        user_id=user_id, \n",
    "        from_timestamp=target_day, \n",
    "        to_timestamp=target_day\n",
    "    )\n",
    "    \n",
    "    if not score_df.empty:\n",
    "        score_df = score_df[[\"id\",\"timestamp\",\"name\",\"value\",\"comment\",\"traceId\",\"observationId\",\"trace\"]]\n",
    "        score_df.to_csv(f\"data/yui/score/{target_day}.csv\", index=False, quoting=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>name</th>\n",
       "      <th>value</th>\n",
       "      <th>comment</th>\n",
       "      <th>traceId</th>\n",
       "      <th>observationId</th>\n",
       "      <th>trace</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36b9fa41-f35d-4631-ab99-b8b7b07054a2</td>\n",
       "      <td>2025-04-15T08:27:59.669Z</td>\n",
       "      <td>小山 舞(Mai Koyama)</td>\n",
       "      <td>-1</td>\n",
       "      <td>Type: その他 - PCの故障交換をお願いいたします。</td>\n",
       "      <td>5229a702-228b-4cc4-b8da-74e9cb009e78</td>\n",
       "      <td>None</td>\n",
       "      <td>{'userId': '小山 舞(Mai Koyama)', 'tags': []}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bbe83222-17a8-4ec1-af20-f28cf60024b9</td>\n",
       "      <td>2025-04-15T03:25:43.938Z</td>\n",
       "      <td>伊藤 和也(Kazuya Ito)</td>\n",
       "      <td>1</td>\n",
       "      <td>LIKE</td>\n",
       "      <td>18199780-0cbb-46b4-96e8-e073f307a216</td>\n",
       "      <td>None</td>\n",
       "      <td>{'userId': '伊藤 和也(Kazuya Ito)', 'tags': []}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b2f2f6e2-b328-4c8a-afb8-edb7d7538f18</td>\n",
       "      <td>2025-04-14T20:55:55.751Z</td>\n",
       "      <td>小室 幸大(Kodai Komuro)</td>\n",
       "      <td>1</td>\n",
       "      <td>LIKE</td>\n",
       "      <td>4a9b5ae1-efd8-4b82-a966-e136d5f9862b</td>\n",
       "      <td>None</td>\n",
       "      <td>{'userId': '小室 幸大(Kodai Komuro)', 'tags': []}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id                 timestamp  \\\n",
       "0  36b9fa41-f35d-4631-ab99-b8b7b07054a2  2025-04-15T08:27:59.669Z   \n",
       "1  bbe83222-17a8-4ec1-af20-f28cf60024b9  2025-04-15T03:25:43.938Z   \n",
       "2  b2f2f6e2-b328-4c8a-afb8-edb7d7538f18  2025-04-14T20:55:55.751Z   \n",
       "\n",
       "                  name  value                        comment  \\\n",
       "0     小山 舞(Mai Koyama)     -1  Type: その他 - PCの故障交換をお願いいたします。   \n",
       "1    伊藤 和也(Kazuya Ito)      1                           LIKE   \n",
       "2  小室 幸大(Kodai Komuro)      1                           LIKE   \n",
       "\n",
       "                                traceId observationId  \\\n",
       "0  5229a702-228b-4cc4-b8da-74e9cb009e78          None   \n",
       "1  18199780-0cbb-46b4-96e8-e073f307a216          None   \n",
       "2  4a9b5ae1-efd8-4b82-a966-e136d5f9862b          None   \n",
       "\n",
       "                                           trace  \n",
       "0     {'userId': '小山 舞(Mai Koyama)', 'tags': []}  \n",
       "1    {'userId': '伊藤 和也(Kazuya Ito)', 'tags': []}  \n",
       "2  {'userId': '小室 幸大(Kodai Komuro)', 'tags': []}  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trace data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-18 10:36:46,360 - root - INFO - Collect data from JST 2025-04-14 00:00:00+09:00 to 2025-04-14 23:59:59.999999+09:00\n",
      "2025-04-18 10:36:47,128 - root - INFO - Page#10, 2025-04-14 01:58:44.824000+00:00 ~ 2025-04-14 01:27:10.884000+00:00\n",
      "2025-04-18 10:36:47,297 - root - INFO - All traces have been collected as the number of records is less than limit 100 per page.\n",
      "2025-04-18 10:36:47,298 - root - INFO - Page#13, 2025-04-13 22:15:51.258000+00:00 ~ 2025-04-13 15:00:15.616000+00:00\n",
      "2025-04-18 10:36:47,299 - root - INFO - Retrieved 1212 traces.\n"
     ]
    }
   ],
   "source": [
    "limit = 100\n",
    "page = 1\n",
    "max_traces = None\n",
    "user_id = None\n",
    "metadata_keys = None\n",
    "\n",
    "dts = [\n",
    "    \"2025-04-14\"\n",
    "]\n",
    "\n",
    "for from_timestamp in dts:\n",
    "\n",
    "    trace_df = langfuseapi.fetch_traces(\n",
    "        limit=limit, \n",
    "        page=page, \n",
    "        user_id=user_id, \n",
    "        from_timestamp=from_timestamp, \n",
    "        to_timestamp=from_timestamp,\n",
    "        metadata_keys=metadata_keys\n",
    "    )\n",
    "    trace_df = trace_df[[\"id\",\"name\",\"timestamp\",'input', 'output', \"userId\",\"scores\",\"version\",\"azure_user_id\",\"question_uuid\",\"department_name\",\"using\", \"channel\",'costs', 'audio_length','user_email', 'totalCost', 'observations']]\n",
    "    trace_df.to_csv(f\"data/yui/trace/{from_timestamp}.csv\", index=False, quoting=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langfuse-v2-hMTK38-U-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
